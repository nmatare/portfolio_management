outMSE[i] = MSE
}
#outMSE
#plot
plot(log(1/kvec),sqrt(outMSE),ylim=c(1,1.24))#,ylim=c(min(sqrt(outMSE),sqrt(mean((test$x_train-y_test_predicted)^2))),max(sqrt(outMSE),sqrt(mean((test$x_train-y_test_predicted)^2)))))
imin = which.min(outMSE)
#cat("best k is ",kvec[imin],"\n") #9 is best, agreed
y_test_predicted=predict(simple.lm,newdata=test,type='response')
#sqrt(mean((y_test_created-y_test_predicted)^2))
abline(h=sqrt(mean((y_test_created-y_test_predicted)^2)),lty=2) #Fixed! Hooray
```
\newpage
## 1.8
The superfluous features have no predictive power. Thus, when the amount of noise increases in the dataset, the Knn algorithm uses spurious features to predict the value of y. If K is small and the number of superfluous features is large, then there is a high likelihood that the algorithm uses many erroneous covariates to attempt to predict y. As K increases, the algorithm uses more features to predict y. That is, as the likelihood of the number of spurious features decreases, giving a lower MSE. Please see the below graphs depicting the decrease in MSE as the amount of noise in the dataset increases. KNN, at all values of K, decreases in accuracy as the amount of noise increases.
```{r question1.8, echo=FALSE, eval=TRUE, include=TRUE, fig.height=6, fig.align='center', message=FALSE}
current.relation = function(x,e){
y=sin(2*x)+2+e
return(y)
}
par(mfrow=c(4,5))
#First I'm rerunning 1.7
x_train = rnorm(100)
epsilon = rnorm(100)
y_train_created = current.relation(x_train,epsilon)
x_test = rnorm(10000)
x_test=x_test[order(x_test)]
epsilon_test = rnorm(10000)
y_test_created = current.relation(x_test,epsilon_test)
simple.lm=lm(y_train_created~x_train)
train=data.frame(y_train_created,x_train)
test=data.frame(x_train=sort(x_test))
#Should be the end of plot
#loop over values of k, fit on train, predict on test
kvec=2:15; nk=length(kvec)
outMSE = rep(0,nk) #will will put the out-of-sample MSE here
for(i in 1:nk) {
near = kknn(y_train_created~x_train,train,test,k=kvec[i],kernel = "rectangular")
MSE = mean((y_test_created-near$fitted.values)^2)
outMSE[i] = MSE
}
plot(log(1/kvec),sqrt(outMSE),main="p = 1",ylim=c(1,1.5))#,ylim=c(min(sqrt(outMSE),sqrt(mean((test$x_train-y_test_predicted)^2))),max(sqrt(outMSE),sqrt(mean((test$x_train-y_test_predicted)^2)))))
y_test_predicted=predict(simple.lm,newdata=test,type='response')
abline(h=sqrt(mean((y_test_created-y_test_predicted)^2)),lty=2) #Fixed! Hooray
#Now I do it for all p's
for(p in 2:20)
{
x_1 = rnorm(100)
epsilon = rnorm(100)
y_train_created = current.relation(x_1,epsilon)
x_train=data.frame(x_1)
for(i in 2:p){
temp.names=names(x_train)
x_train=data.frame(x_train,rnorm(100))
names(x_train)=c(temp.names,paste0("x_",i))
}
head(x_train)
x_1 = rnorm(10000)
epsilon_test = rnorm(10000)
x_test=data.frame(x_1)
for(i in 2:p){
temp.names=names(x_test)
x_test=data.frame(x_test,rnorm(10000))
names(x_test)=c(temp.names,paste0("x_",i))
}
head(x_test)
y_test_created = current.relation(x_1,epsilon_test)
#plot(x_train,y_train_created)
#x=1:10000/1000-5
#y=current.relation(x,0)
#lines(x,y)
full.data=data.frame(y_train_created,x_train)
simple.lm=lm(y_train_created~.,data=full.data)
train=data.frame(y_train_created,x_train)
test=x_test
#kf2 = kknn(y_train_created~.,train,test,k=2,kernel = "rectangular")
#lines(test$x_train,kf2$fitted.values,col="blue",lwd=2)
#kf12 = kknn(y_train_created~.,train,test,k=12,kernel = "rectangular")
#lines(test$x_train,kf12$fitted.values,col="red",lwd=2)
#legend("topleft",fill=c("blue","red","green"),c("k=2","k=12","linear"))
#loop over values of k, fit on train, predict on test
kvec=2:15; nk=length(kvec)
outMSE = rep(0,nk) #will will put the out-of-sample MSE here
for(i in 1:nk) {
near = kknn(y_train_created~.,train,test,k=kvec[i],kernel = "rectangular")
MSE = mean((y_test_created-near$fitted.values)^2)
outMSE[i] = MSE
}
#outMSE
#plot
y_test_predicted=predict(simple.lm,newdata=test,type='response')
plot(log(1/kvec),sqrt(outMSE),main=p,ylim=c(1,1.5))#c(min(sqrt(outMSE),sqrt(mean((y_test_created-y_test_predicted)^2))),max(sqrt(outMSE),sqrt(mean((y_test_created-y_test_predicted)^2)))))
#imin = which.min(outMSE)
#cat("best k is ",kvec[imin],"\n")
abline(h=sqrt(mean((y_test_created-y_test_predicted)^2)),lty=2) #Fixed! Hooray
}
```
\newpage
## Bonus
Holding the amount of noise fixed, as the training dataset increases in size, the likelihood of superfluous features chosen to predict y decreases; there is a greater likelihood that KNN will select features with true predictive power as opposed to simple noise. As before, holding the amount of noise fixed, as K increases, the algorithm uses more features to predict y-hat. Thus, the likelihood that the chosen features are spurious decreases, giving a lower MSE. See the below graphs: the first set of graphs shows a training dataset of 100 while the second set of graphs show a training dataset of 1000. Noise is held constant in both graphs at five (Five columns of random features)
```{r question1bonus, echo=FALSE, eval=TRUE, include=TRUE, fig.height=2, fig.align='center', message=FALSE}
q1 <- doQuestionOne(num.train = 100, noise = 5, equation = quote(2 + sin(2 * x))) # given training dataset with 100
grid.arrange(q1$plot2)
q2 <- doQuestionOne(num.train = 1000, noise = 5, equation = quote(2 + sin(2 * x))) # given training dataset with 1000
grid.arrange(q2$plot2)
```
# Problem 2
## 2.1
## 2.2
We split the data into two parts: a training and testing set.
```{r question2.2, include=TRUE, fig.height=6, fig.align='center', cache=TRUE}
set.seed(1)
sample.index=sample(nrow(used.cars),nrow(used.cars)/4)
used.cars.test=used.cars[sample.index,]
used.cars.train=used.cars[-sample.index,]
```
used.cars
car.lm=lm(price ~ mileage,data=used.cars.train)
par(mfrow=c(1,1))
plot(used.cars.train$mileage,used.cars.train$price,pch='.',xlab="Mileage",ylab="Price")
abline(car.lm)
n_folds=10
n_dimensions=10
avgMSE = rep(0,n_dimensions)
for (d in 1:n_dimensions){
############## NOTE - maybe move the folds outside
outMSE = rep(0,n_folds) #will will put the out-of-sample MSE here
folds=sample(rep(1:n_folds,length.out=nrow(used.cars.train)))
for(i in 1:n_folds)
{
used.cars.train.current=used.cars.train[folds!=folds[i],]
current.lm=lm(price ~ poly(mileage,d),data=used.cars.train.current)
predicted.price=predict(current.lm,used.cars.train[folds==folds[i],])
outMSE[i]=mean((predicted.price-used.cars.train[folds==folds[i],]$price)^2)
}
avgMSE[d]=mean(outMSE)
}
plot(1:n_dimensions,sqrt(avgMSE))
#which.min(sqrt(avgMSE))
#choosing 5
degree=5
poly.lm=lm(price ~ poly(mileage,degree),data=used.cars.train)
x.plot.data=1:4000*100
y.plot.data=predict(poly.lm,newdata=data.frame(mileage=x.plot.data))
plot(used.cars.train$mileage,used.cars.train$price,pch='.',xlab="Mileage",ylab="Price")
lines(x.plot.data,y.plot.data,col="blue")
#2.5
#docvknn(matrix x, vector y,vector of k values, number of folds),
kv=1:10*100
#does cross-validation for training data (x,y).
cv1 = docvknn(matrix(used.cars.train$mileage),used.cars.train$price,kv,nfold=5)
cv2 = docvknn(matrix(used.cars.train$mileage),used.cars.train$price,kv,nfold=5)
cv3 = docvknn(matrix(used.cars.train$mileage),used.cars.train$price,kv,nfold=10)
#docvknn returns error sum of squares, want RMSE
cv1 = sqrt(cv1)/length(used.cars.train)
cv2 = sqrt(cv2)/length(used.cars.train)
cv3 = sqrt(cv3)/length(used.cars.train)
rgy = range(c(cv1,cv2,cv3))
plot(log(1/kv),cv1,type="l",col="red",ylim=rgy,lwd=2,cex.lab=2.0, xlab="log(1/k)", ylab="RMSE")
lines(log(1/kv),cv2,col="blue",lwd=2)
lines(log(1/kv),cv3,col="green",lwd=2)
legend("topleft",legend=c("5-fold 1","5-fold 2","10 fold"),
col=c("red","blue","green"),lwd=2,cex=1.5)
cv = (cv1+cv2+cv3)/3 #use average
kbest = kv[which.min(cv)]
cat("the best k is: ",kbest,"\n")
#fit kNN with best k and plot the fit.
kfbest = kknn(price~mileage,used.cars.train,used.cars.test[order(used.cars.test$mileage),],
k=kbest,kernel = "rectangular")
plot(used.cars.test$mileage,used.cars.test$price,pch=".")
lines(used.cars.test[order(used.cars.test$mileage),]$mileage,kfbest$fitted,col="red",lwd=2,cex.lab=2)
#Now, for the trees
df2=used.cars.train[,c(1,4)] # pick off dis,lstat,medv
print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
rpart.plot(best.tree)
tree.fit=predict(best.tree)
plot(used.cars.test$mileage,used.cars.test$price,pch=".")
lines(x.plot.data,y.plot.data,col="blue")
lines(used.cars.test[order(used.cars.test$mileage),]$mileage,kfbest$fitted,col="red",lwd=2,cex.lab=2)
lines(used.cars.train$mileage[order(used.cars.train$mileage)],tree.fit[order(used.cars.train$mileage)],col="green")
#I would use the Knn I think - and it has the best MSE!
#knn RMSE
sqrt(sum((used.cars.test[order(used.cars.test$mileage),]$price-kfbest$fitted)^2))/nrow(used.cars.test)
#poly RMSE
poly.predict=predict(poly.lm,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-poly.predict)^2))/nrow(used.cars.test)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
df2
used.cars.train
df2=used.cars.train[,c(1,4), with = FALSE] # pick off dis,lstat,medv
print(names(df2))
df2
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cv = (cv1+cv2+cv3)/3 #use average
kbest = kv[which.min(cv)]
cat("the best k is: ",kbest,"\n")
#fit kNN with best k and plot the fit.
kfbest = kknn(price~mileage,used.cars.train,used.cars.test[order(used.cars.test$mileage),],
k=kbest,kernel = "rectangular")
plot(used.cars.test$mileage,used.cars.test$price,pch=".")
lines(used.cars.test[order(used.cars.test$mileage),]$mileage,kfbest$fitted,col="red",lwd=2,cex.lab=2)
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
rpart.plot(best.tree)
tree.fit=predict(best.tree)
plot(used.cars.test$mileage,used.cars.test$price,pch=".")
lines(x.plot.data,y.plot.data,col="blue")
lines(used.cars.test[order(used.cars.test$mileage),]$mileage,kfbest$fitted,col="red",lwd=2,cex.lab=2)
lines(used.cars.train$mileage[order(used.cars.train$mileage)],tree.fit[order(used.cars.train$mileage)],col="green")
#I would use the Knn I think - and it has the best MSE!
#knn RMSE
sqrt(sum((used.cars.test[order(used.cars.test$mileage),]$price-kfbest$fitted)^2))/nrow(used.cars.test)
#poly RMSE
poly.predict=predict(poly.lm,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-poly.predict)^2))/nrow(used.cars.test)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
library(rpart)
rpart.plot(best.tree)
rpart::rpart.plot(best.tree)
#knn RMSE
sqrt(sum((used.cars.test[order(used.cars.test$mileage),]$price-kfbest$fitted)^2))/nrow(used.cars.test)
#poly RMSE
poly.predict=predict(poly.lm,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-poly.predict)^2))/nrow(used.cars.test)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
tree.fit=predict(best.tree)
plot(used.cars.test$mileage,used.cars.test$price,pch=".")
lines(x.plot.data,y.plot.data,col="blue")
lines(used.cars.test[order(used.cars.test$mileage),]$mileage,kfbest$fitted,col="red",lwd=2,cex.lab=2)
lines(used.cars.train$mileage[order(used.cars.train$mileage)],tree.fit[order(used.cars.train$mileage)],col="green")
#I would use the Knn I think - and it has the best MSE!
#knn RMSE
sqrt(sum((used.cars.test[order(used.cars.test$mileage),]$price-kfbest$fitted)^2))/nrow(used.cars.test)
#poly RMSE
poly.predict=predict(poly.lm,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-poly.predict)^2))/nrow(used.cars.test)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#first, knn
x = cbind(used.cars.train$mileage,used.cars.train$year)
colnames(x) = c("mileage","year")
y = used.cars.train$price
mmsc=function(x) {return((x-min(x))/(max(x)-min(x)))}
xs = apply(x,2,mmsc) #apply scaling function to each column of x
#plot y vs each x
par(mfrow=c(1,2)) #two plot frames
plot(x[,1],y,xlab="mileage",ylab="price")
plot(x[,2],y,xlab="year",ylab="price")
#run cross val once
par(mfrow=c(1,1))
set.seed(99)
kv = 1:12*10 #k values to try
n = length(y)
cvtemp = docvknn(xs,y,kv,nfold=10)
cvtemp = sqrt(cvtemp/n) #docvknn returns sum of squares
plot(kv,cvtemp)
#refit using all the data and k=80
ddf = data.frame(y,xs)
near5 = kknn(y~.,ddf,ddf,k=80,kernel = "rectangular")
lmf = lm(y~.,ddf)
fmat = cbind(y,near5$fitted,lmf$fitted)
colnames(fmat)=c("y","kNN5","linear")
pairs(fmat)
print(cor(fmat))
#knn RMSE
test.x=data.frame(used.cars.test[,c(4,5)])
names(test.x)=c("mileage","year")
adjusted.test.x=data.frame(apply(test.x,2,mmsc))
kfbest=kknn(y~.,ddf,adjusted.test.x,k=80,kernel = "rectangular")
sqrt(sum((used.cars.test$price-kfbest$fitted)^2))/nrow(used.cars.test)
test.x=data.frame(used.cars.test[,c(4,5)])
test.x
used.cars.test
test.x=data.frame(used.cars.test[,c(4,5), with = FALSE])
names(test.x)=c("mileage","year")
adjusted.test.x=data.frame(apply(test.x,2,mmsc))
kfbest=kknn(y~.,ddf,adjusted.test.x,k=80,kernel = "rectangular")
sqrt(sum((used.cars.test$price-kfbest$fitted)^2))/nrow(used.cars.test)
#now trees
df2=used.cars.train[,c(1,4,5)] # pick off columns
print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
rpart.plot(best.tree)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#yes, performs better and also both ks go down
nbig = length(unique(big.tree$where))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
df2
#now trees
df2=used.cars.train[,c(1,4,5), with = FALSE] # pick off columns
print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
rpart.plot(best.tree)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#yes, performs better and also both ks go down
#2.7
big.tree = rpart(price~., data=used.cars.train,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree,ylim=c(0.05,.08)) # plot results
best.tree = prune(big.tree,cp=bestcp)
#rpart.plot(best.tree)
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#first, knn
x = cbind(used.cars.train$mileage,used.cars.train$year)
colnames(x) = c("mileage","year")
y = used.cars.train$price
mmsc=function(x) {return((x-min(x))/(max(x)-min(x)))}
xs = apply(x,2,mmsc) #apply scaling function to each column of x
#plot y vs each x
par(mfrow=c(1,2)) #two plot frames
plot(x[,1],y,xlab="mileage",ylab="price")
plot(x[,2],y,xlab="year",ylab="price")
#run cross val once
par(mfrow=c(1,1))
set.seed(99)
kv = 1:12*10 #k values to try
n = length(y)
cvtemp = docvknn(xs,y,kv,nfold=10)
cvtemp = sqrt(cvtemp/n) #docvknn returns sum of squares
plot(kv,cvtemp)
#refit using all the data and k=80
ddf = data.frame(y,xs)
near5 = kknn(y~.,ddf,ddf,k=80,kernel = "rectangular")
lmf = lm(y~.,ddf)
fmat = cbind(y,near5$fitted,lmf$fitted)
colnames(fmat)=c("y","kNN5","linear")
pairs(fmat)
print(cor(fmat))
#knn RMSE
test.x=data.frame(used.cars.test[,c(4,5), with = FALSE])
names(test.x)=c("mileage","year")
adjusted.test.x=data.frame(apply(test.x,2,mmsc))
kfbest=kknn(y~.,ddf,adjusted.test.x,k=80,kernel = "rectangular")
sqrt(sum((used.cars.test$price-kfbest$fitted)^2))/nrow(used.cars.test)
#now trees
df2=used.cars.train[,c(1,4,5), with = FALSE] # pick off columns
print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
rpart.plot(best.tree)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#yes, performs better and also both ks go down
cat("the best k is: ",kbest,"\n")
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
bestcp
#knn RMSE
sqrt(sum((used.cars.test[order(used.cars.test$mileage),]$price-kfbest$fitted)^2))/nrow(used.cars.test)
#poly RMSE
poly.predict=predict(poly.lm,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-poly.predict)^2))/nrow(used.cars.test)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
bestcp
#now trees
df2=used.cars.train[,c(1,4,5), with = FALSE] # pick off columns
print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = big.tree$cptable
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
bestcp
#Now, for the trees
df2=used.cars.train[,c(1,4), with = FALSE] # pick off dis,lstat,medv
#print(names(df2))
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = big.tree$cptable
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
bestcp
#2.7
big.tree = rpart(price~., data=used.cars.train,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = printcp(big.tree)
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree,ylim=c(0.05,.08)) # plot results
best.tree = prune(big.tree,cp=bestcp)
#rpart.plot(best.tree)
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
# create a big tree
big.tree = rpart(price~., data=df2,
control=rpart.control(minsplit=5,
cp=0.0001,
xval=10)
)
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')
cptable = big.tree$cptable
bestcp = cptable[ which.min(cptable[,"xerror"]), "CP" ]   # this is the optimal cp parameter
plotcp(big.tree) # plot results
best.tree = prune(big.tree,cp=bestcp)
# rpart.plot(best.tree)
#tree RMSE
new.fit=predict(best.tree,newdata=used.cars.test)
sqrt(sum((used.cars.test$price-new.fit)^2))/nrow(used.cars.test)
#yes, performs better and also both ks go down
nbig
options("width" = 250)
options(scipen  = 999)
options(digits  = 003)
library(xts); library(zoo); library(e1071); library(ggplot2); library(knitr)
set.seed(666) # the devils seed
username <- Sys.info()[["user"]]
dir <- paste("/home/", username, "/Documents/Education/Chicago_Booth/Classes/35120_Portfolio_Management/portfolio_managment/homework1", sep = "")
setwd(dir)
options("width" = 250)
options(scipen  = 999)
options(digits  = 003)
library(xts); library(zoo); library(e1071); library(ggplot2); library(knitr)
set.seed(666) # the devils seed
username <- Sys.info()[["user"]]
dir <- paste("/home/", username, "/Documents/Education/Chicago_Booth/Classes/35120_Portfolio_Management/portfolio_management/homework1", sep = "")
setwd(dir)
